Most of the efforts related to video models seem to be focused on creating new architectures that resolve the current challenges or solving generic tasks like human action recognition.

Notable examples of these new architectures include ViT-based models like TimeSFormer, proposed by Bertasius et al. \cite{DBLP:journals/corr/abs-2102-05095}, and ViVit, proposed by Arnab et al. \cite{DBLP:journals/corr/abs-2103-15691}. On the other hand, significant advancements have also emerged from autoencoder architectures that employ masking techniques, such as VideoMAE, developed by Tong et al. \cite{VideoMAE}. More recently, Li et al. \cite{li2024videomambastatespacemodel} created their videoMamba model by adapting linear-complexity operators, opening the field to more cost effective alternatives.

Based on our research, there have not been many applications of video models in medical context. The only example found was the study by Howard et al. \cite{JMAI5205} where they analyzed the potential of different CNN architectures to classify echocardiography ultrasound videos. In this study, the authors compared traditional image-based CNNs, image CNNs with a time-distributed layer, 3D CNNs, and 'two-stream' (spatial and temporal) CNNs. Their findings showed that a two-stream CNN achieved the lowest error rate, suggesting that incorporating the temporal dimension could enhance the accuracy of automatic classification \cite{JMAI5205}.