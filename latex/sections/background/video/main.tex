Video models are deep learning models designed to automatically categorize video content, a task commonly referred to as video classification. These models expand traditional image classification by analyzing sequences of frames instead of single images \cite{electronics13142732}. Typically, they are trained using supervised learning, relying on large-scale datasets to detect patterns and make predictions, such as Kinetics, Something-Something, YouTube-8M, Moments in Time, EPIC-KITCHENS, or others  \cite{electronics13142732}.

Initially, these models were dominated by repurposed 2D image Convolutional Neural Networks (CNNs), which processed only the spatial information within each frame \cite{electronics13142732, DBLP:journals/corr/abs-2103-15691}. Over time, they evolved to incorporate 3D convolutions, recurrent techniques, hybrid methods, or attention mechanisms to capture the temporal dimension as well, leading to the development of 3D CNNs \cite{electronics13142732}. More recent advancements have explored Vision Transformers (ViTs) as an alternative to CNNs, yielding promising results in both computational cost and accurate predictions \cite{electronics13142732, DBLP:journals/corr/abs-2103-15691, DBLP:journals/corr/abs-2102-05095}.

Video model processing typically follows one of two approaches: single-frame or fusion-based. In the single-frame approach, a video is broken down into individual frames, with each frame treated as an independent image. This method simplifies the process by ignoring temporal information, making it less resource-intensive \cite{electronics13142732}. In contrast, fusion-based approaches handle multiple modalities, such as visual, audio, and text data, with the method of processing varying based on the type of fusion performed \cite{electronics13142732}.

These fusion types share a certain similarity in both name and process with the ones from multiview models, however they differentiate mainly through the use of modalities. In this case, early fusion extracts features from each modality and combines them into a unified representation before passing it through a classifier \cite{electronics13142732}. Late fusion, on the other hand, processes each modality separately through specialized classifiers, and their predictions are combined at the end \cite{electronics13142732}. Slow fusion blends aspects of both strategies by gradually integrating modalities at multiple stages throughout the architecture, either through feature extraction or prediction making \cite{electronics13142732}.