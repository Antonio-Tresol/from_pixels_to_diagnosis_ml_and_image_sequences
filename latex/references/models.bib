%=======================FOCUS ON MODELS=========================

% We present a convolution-free approach to video classification built exclusively on self-attention over space and time. Our method, named "TimeSformer," adapts the standard Transformer architecture to video by enabling spatiotemporal feature learning directly from a sequence of frame-level patches. Our experimental study compares different self-attention schemes and suggests that "divided attention," where temporal attention and spatial attention are separately applied within each block, leads to the best video classification accuracy among the design choices considered. Despite the radically new design, TimeSformer achieves state-of-the-art results on several action recognition benchmarks, including the best reported accuracy on Kinetics-400 and Kinetics-600. Finally, compared to 3D convolutional networks, our model is faster to train, it can achieve dramatically higher test efficiency (at a small drop in accuracy), and it can also be applied to much longer video clips (over one minute long) https://github.com/facebookresearch/TimeSformer (added by Antonio)
@article{DBLP:journals/corr/abs-2102-05095,
  author       = {Gedas Bertasius and
                  Heng Wang and
                  Lorenzo Torresani},
  title        = {Is Space-Time Attention All You Need for Video Understanding?},
  journal      = {CoRR},
  volume       = {abs/2102.05095},
  year         = {2021},
  url          = {https://arxiv.org/abs/2102.05095},
  eprinttype    = {arXiv},
  eprint       = {2102.05095},
  timestamp    = {Thu, 18 Feb 2021 15:26:00 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2102-05095.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

%We present pure-transformer based models for video classification, drawing upon the recent success of such models in image classification. Our model extracts spatio-temporal tokens from the input video, which are then encoded by a series of transformer layers. In order to handle the long sequences of tokens encountered in video, we propose several, efficient variants of our model which factorise the spatial- and temporal-dimensions of the input. Although transformer-based models are known to only be effective when large training datasets are available, we show how we can effectively regularise the model during training and leverage pretrained image models to be able to train on comparatively small datasets. We conduct thorough ablation studies, and achieve state-of-the-art results on multiple video classification benchmarks including Kinetics 400 and 600, Epic Kitchens, Something-Something v2 and Moments in Time, outperforming prior methods based on deep 3D convolutional networks. To facilitate further research, we release code at this https URL
% the newest itereation of the timesformer, essentially (added by Antonio) 

@article{DBLP:journals/corr/abs-2103-15691,
  author       = {Anurag Arnab and
                  Mostafa Dehghani and
                  Georg Heigold and
                  Chen Sun and
                  Mario Lucic and
                  Cordelia Schmid},
  title        = {ViViT: {A} Video Vision Transformer},
  journal      = {CoRR},
  volume       = {abs/2103.15691},
  year         = {2021},
  url          = {https://arxiv.org/abs/2103.15691},
  eprinttype    = {arXiv},
  eprint       = {2103.15691},
  timestamp    = {Mon, 12 Apr 2021 13:42:11 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2103-15691.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

%traditional deep learning approaches for breast cancer classification has predominantly concentrated on single-view analysis. In clinical practice, however, radiologists concurrently examine all views within a mammography exam, leveraging the inherent correlations in these views to effectively detect tumors. Acknowledging the significance of multi-view analysis, some studies have introduced methods that independently process mammogram views, either through distinct convolutional branches or simple fusion strategies, inadvertently leading to a loss of crucial inter-view correlations. In this paper, we propose an innovative multi-view network exclusively based on transformers to address challenges in mammographic image classification. Our approach introduces a novel shifted window-based dynamic attention block, facilitating the effective integration of multi-view information and promoting the coherent transfer of this information between views at the spatial feature map level. Furthermore, we conduct a comprehensive comparative analysis of the performance and effectiveness of transformer-based models under diverse settings, employing the CBIS-DDSM and Vin-Dr Mammo datasets. Our code is publicly available at this https URL (added by Antonio)
@misc{sarker2024mvswintmammogramclassificationmultiview,
      title={MV-Swin-T: Mammogram Classification with Multi-view Swin Transformer}, 
      author={Sushmita Sarker and Prithul Sarker and George Bebis and Alireza Tavakkoli},
      year={2024},
      eprint={2402.16298},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2402.16298}, 
}

% Pre-training video transformers on extra large-scale datasets is generally required to achieve premier performance on relatively small datasets. In this paper, we show that video masked autoencoders (VideoMAE) are data-efficient learners for self-supervised video pre-training (SSVP). We are inspired by the recent ImageMAE and propose customized video tube masking with an extremely high ratio. This simple design makes video reconstruction a more challenging self-supervision task, thus encouraging extracting more effective video representations during this pre-training process. We obtain three important findings on SSVP: (1) An extremely high proportion of masking ratio (i.e., 90% to 95%) still yields favorable performance of VideoMAE. The temporally redundant video content enables a higher masking ratio than that of images. (2) VideoMAE achieves impressive results on very small datasets (i.e., around 3k-4k videos) without using any extra data. (3) VideoMAE shows that data quality is more important than data quantity for SSVP. Domain shift between pre-training and target datasets is an important issue. Notably, our VideoMAE with the vanilla ViT can achieve 87.4% on Kinetics-400, 75.4% on Something-Something V2, 91.3% on UCF101, and 62.6% on HMDB51, without using any extra data. (Added by Kenneth)
@article{VideoMAE,
  author       = {Zhan Tong and
                  Yibing Song and
                  Jue Wang and
                  Limin Wang},
  title        = {VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training},
  journal      = {CoRR},
  volume       = {abs/2203.12602},
  year         = {2022},
  url          = {https://arxiv.org/abs/2203.12602},
  eprinttype    = {arXiv},
  eprint       = {2203.12602},
  timestamp    = {Tue, 18 Oct 2022 09:15:24 UTC},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2203-12602.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

% Contrastive language-image pretraining has shown great success in learning visual-textual joint representation from web-scale data, demonstrating remarkable "zero-shot" generalization ability for various image tasks. However, how to effectively expand such new language-image pretraining methods to video domains is still an open problem. In this work, we present a simple yet effective approach that adapts the pretrained language-image models to video recognition directly, instead of pretraining a new model from scratch. More concretely, to capture the long-range dependencies of frames along the temporal dimension, we propose a cross-frame attention mechanism that explicitly exchanges information across frames. Such module is lightweight and can be plugged into pretrained language-image models seamlessly. Moreover, we propose a video-specific prompting scheme, which leverages video content information for generating discriminative textual prompts. Extensive experiments demonstrate that our approach is effective and can be generalized to different video recognition scenarios. In particular, under fully-supervised settings, our approach achieves a top-1 accuracy of 87.1% on Kinectics-400, while using 12 times fewer FLOPs compared with Swin-L and ViViT-H. In zero-shot experiments, our approach surpasses the current state-of-the-art methods by +7.6% and +14.9% in terms of top-1 accuracy under two popular protocols. In few-shot scenarios, our approach outperforms previous best methods by +32.1% and +23.1% when the labeled data is extremely limited. (Added by Kenneth)
@article{Expanding-Models,
  author       = {Bolin Ni and
                  Houwen Peng and
                  Minghao Chen and
                  Songyang Zhang and
                  Gaofeng Meng and
                  Jianlong Fu and
                  Shiming Xiang and
                  Haibin Ling},
  title        = {Expanding Language-Image Pretrained Models for General Video Recognition},
  journal      = {CoRR},
  volume       = {abs/2208.02816},
  year         = {2022},
  url          = {https://arxiv.org/abs/2208.02816},
  eprinttype    = {arXiv},
  eprint       = {2208.02816},
  timestamp    = {Tue, 4 Aug 2022 17:59:54 UTC},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2208-02816.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{hybrid_mammo_net,
  author={Allaoui, Hind and Alj, Youssef and Ameskine, Yassine},
  booktitle={2024 IEEE 12th International Symposium on Signal, Image, Video and Communications (ISIVC)}, 
  title={HybridMammoNet: A Hybrid CNN-ViT Architecture for Multi-view Mammography Image Classification}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  keywords={Solid modeling;Limiting;Computational modeling;Merging;Computer architecture;Transformers;Mammography;Mammography;Vision Transformers;Attention;Convolutional Neural Network;Image classification},
  doi={10.1109/ISIVC61350.2024.10577856}}

(added by Antonio)
@inproceedings{10.1145/3503161.3547841,
author = {Tan, Yi and Hao, Yanbin and Zhang, Hao and Wang, Shuo and He, Xiangnan},
title = {Hierarchical Hourglass Convolutional Network for Efficient Video Classification},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3547841},
doi = {10.1145/3503161.3547841},
abstract = {Videos naturally contain dynamic variation over the temporal axis, which will result in the same visual clues (e.g., semantics, objects) changing their scale, position, and perspective patterns between adjacent frames. A primary trend in video CNN is adopting spatial-2D convolution for spatial semantics and temporal-1D convolution for temporal dynamics. Though the direction achieves a favorable balance between efficiency and efficacy, it suffers from misalignment of visual clues with large displacements. Particularly, rigid temporal convolution would fail to capture correct motions when a specific target moves out of the reception field of temporal convolution between adjacent frames.To tackle large visual displacements between temporal neighbors, we propose a new temporal convolution namedHourglass Convolution (HgC). The temporal reception field of HgC has an hourglass shape, where the spatial reception field is enlarged in prior \& post temporal frames, enabling an ability to capture large displacement. Moreover, since videos contain long, short-term movements viewed from multiple temporal interval levels, we hierarchically organize the HgC net to both capture temporal dynamics from frame (short-term) and clip (long-term) levels. Besides, we also adopt strategies, such as low-resolution for short-term modeling and channel reduction for long-term modeling, from efficiency concerns. With HgC, our H$^2$CN equips off-the-shelf CNNs with a strong ability in capturing spatio-temporal dynamics at a neglectable computation overhead. We validate the efficiency and efficacy of HgC on standard action recognition benchmarks, including Something-Something V1&V2, Diving48, and EGTEA Gaze+. We also analyse the complementarity of frame-level motion and clip-level motion with visualizations. The code and models will be available at https://github.com/ty-97/H2CN.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {5880–5891},
numpages = {12},
keywords = {video classification, neural network, convolution, attention},
location = {Lisboa, Portugal},
series = {MM '22}
}

(added by Kenneth)
@misc{li2024videomambastatespacemodel,
      title={VideoMamba: State Space Model for Efficient Video Understanding}, 
      author={Kunchang Li and Xinhao Li and Yi Wang and Yinan He and Yali Wang and Limin Wang and Yu Qiao},
      year={2024},
      eprint={2403.06977},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2403.06977}, 
}

(added by Antonio)
@INPROCEEDINGS {multiview_hybrid_fusion_mutual_distill,
author = {S. Black and R. Souvenir},
booktitle = {2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
title = {Multi-view Classification Using Hybrid Fusion and Mutual Distillation},
year = {2024},
volume = {},
issn = {},
pages = {269-279},
abstract = {Multi-view classification problems are common in medical image analysis, forensics, and other domains where problem queries involve multi-image input. Existing multi-view classification methods are often tailored to a specific task. In this paper, we repurpose off-the-shelf Hybrid CNN-Transformer networks for multi-view classification with either structured or unstructured views. Our approach incorporates a novel fusion scheme, mutual distillation, and minimal additional parameters. We demonstrate the effectiveness and generalization capability of our approach, MV-HFMD, on multiple multi-view classification tasks and show that it outperforms other multi-view approaches, even task-specific methods. Code is available at https://github.com/vidarlab/multi-view-hybrid.},
keywords = {computer vision;adaptation models;image analysis;codes;forensics;computational modeling;computer architecture},
doi = {10.1109/WACV57701.2024.00034},
url = {https://doi.ieeecomputersociety.org/10.1109/WACV57701.2024.00034},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {jan}
}

@ARTICLE{multiview_a_surview,
  author={Li, Yingming and Yang, Ming and Zhang, Zhongfei},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={A Survey of Multi-View Representation Learning}, 
  year={2019},
  volume={31},
  number={10},
  pages={1863-1883},
  keywords={Correlation;Learning systems;Machine learning;Markov random fields;Neural networks;Data models;Kernel;Multi-view representation learning;canonical correlation analysis;multi-view deep learning},
  doi={10.1109/TKDE.2018.2872063}}

@misc{swint_multiview,
      title={MV-Swin-T: Mammogram Classification with Multi-view Swin Transformer}, 
      author={Sushmita Sarker and Prithul Sarker and George Bebis and Alireza Tavakkoli},
      year={2024},
      eprint={2402.16298},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2402.16298}, 
}

@Article{electronics13142732,
  AUTHOR = {Mao, Makara and Lee, Ahyoung and Hong, Min},
  TITLE = {Deep Learning Innovations in Video Classification: A Survey on Techniques and Dataset Evaluations},
  JOURNAL = {Electronics},
  VOLUME = {13},
  YEAR = {2024},
  NUMBER = {14},
  ARTICLE-NUMBER = {2732},
  URL = {https://www.mdpi.com/2079-9292/13/14/2732},
  ISSN = {2079-9292},
  ABSTRACT = {Video classification has achieved remarkable success in recent years, driven by advanced deep learning models that automatically categorize video content. This paper provides a comprehensive review of video classification techniques and the datasets used in this field. We summarize key findings from recent research, focusing on network architectures, model evaluation metrics, and parallel processing methods that enhance training speed. Our review includes an in-depth analysis of state-of-the-art deep learning models and hybrid architectures, comparing models to traditional approaches and highlighting their advantages and limitations. Critical challenges such as handling large-scale datasets, improving model robustness, and addressing computational constraints are explored. By evaluating performance metrics, we identify areas where current models excel and where improvements are needed. Additionally, we discuss data augmentation techniques designed to enhance dataset accuracy and address specific challenges in video classification tasks. This survey also examines the evolution of convolutional neural networks (CNNs) in image processing and their adaptation to video classification tasks. We propose future research directions and provide a detailed comparison of existing approaches using the UCF-101 dataset, highlighting progress and ongoing challenges in achieving robust video classification.},
  DOI = {10.3390/electronics13142732}
}

@misc{liu2022convnet2020s,
      title={A ConvNet for the 2020s}, 
      author={Zhuang Liu and Hanzi Mao and Chao-Yuan Wu and Christoph Feichtenhofer and Trevor Darrell and Saining Xie},
      year={2022},
      eprint={2201.03545},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2201.03545}, 
}

@INPROCEEDINGS{convnextbro,
  author={Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},
  booktitle={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={A ConvNet for the 2020s}, 
  year={2022},
  volume={},
  number={},
  pages={11966-11976},
  keywords={Computer vision;Image segmentation;Visualization;Computational modeling;Scalability;Semantics;Transformers;Deep learning architectures and techniques; Recognition: detection;categorization;retrieval; Representation learning},
  doi={10.1109/CVPR52688.2022.01167}}


@inproceedings{reddy_enhancing_2024,
	address = {Niagara Falls, ON, Canada},
	title = {Enhancing {Breast} {Cancer} {Detection}: {A} {Novel} {Training} {Strategy} and {Batch} {Scheduler} {Method}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350374285},
	shorttitle = {Enhancing {Breast} {Cancer} {Detection}},
	url = {https://ieeexplore.ieee.org/document/10672621/},
	doi = {10.1109/AVSS61716.2024.10672621},
	abstract = {Recent advancements in deep learning and computational power have opened new possibilities that were once unattainable. Researchers are now eager to transfer their expertise in deep learning to various domains, including medical diagnostics, autonomous systems, and remote sensing. In the medical field, the application of deep learning promises to reduce the workload of healthcare professionals, streamline screening processes, and improve time efficiency. Breast cancer remains a significant challenge for many women, with mass-type cancers having increased difficulty due to their heterogeneity and anomalies. Addressing this issue requires a more detailed investigation. In this study, we propose an innovative training strategy aimed at improving the precision and F1 score of breast cancer detection models. Furthermore, we introduce a novel method, named the Batch Scheduler, which dynamically adjusts batch sizes during the training phase, rather than maintaining a constant size throughout. This approach has been shown to improve the performance of the existing system by 0.4\%. For our training and testing, we used ’ConvNext’ equipped with pre-trained weights, which further contributed to the robustness of our model.},
	language = {en},
	urldate = {2024-10-29},
	booktitle = {2024 {IEEE} {International} {Conference} on {Advanced} {Video} and {Signal} {Based} {Surveillance} ({AVSS})},
	publisher = {IEEE},
	author = {Reddy, Akumalla Brahma and Pham, Bach-Tung and Wang, Jia-Ching},
	month = jul,
	year = {2024},
	pages = {1--6},
	file = {Reddy et al. - 2024 - Enhancing Breast Cancer Detection A Novel Trainin.pdf:C\:\\Users\\anton\\Zotero\\storage\\WB3GBMBP\\Reddy et al. - 2024 - Enhancing Breast Cancer Detection A Novel Trainin.pdf:application/pdf},
}

@inproceedings{nizamli_accurate_2024,
	address = {Saint Petersburg, Russian Federation},
	title = {Accurate {Anomaly} {Detection} in {Medical} {Images} using {Transfer} {Learning} and {Data} {Optimization}: {MRI} and {CT} as {Case} {Studies}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350363739},
	shorttitle = {Accurate {Anomaly} {Detection} in {Medical} {Images} using {Transfer} {Learning} and {Data} {Optimization}},
	url = {https://ieeexplore.ieee.org/document/10585603/},
	doi = {10.1109/NeuroNT62606.2024.10585603},
	abstract = {Today, medical imaging techniques are widely used to detect a variety of human conditions and diseases. To speed up the diagnostic process, systems are often automated using deep learning methods, which have been proven to yield outstanding results. However, these models may be limited to a specific domain, prone to overfitting, and difficult to update. In this paper, we present a transfer learning approach that achieves high accuracy for detecting anomalies in both MRI and CT images. Our system utilizes the ConvNeXt network, inspired by the Swin Transformer architecture, as a high-level feature extractor from processed and augmented medical images. The resulting data is then optimized using the Edited Nearest Neighbors algorithm, which selects the most effective observations and discards the rest. Finally, the data is passed to the K-Nearest Neighbors classifier to detect anomalies in numerically represented images. Our model achieves high performance for two different types of medical images and can easily be updated without the need for retraining.},
	language = {en},
	urldate = {2024-10-29},
	booktitle = {2024 {V} {International} {Conference} on {Neural} {Networks} and {Neurotechnologies} ({NeuroNT})},
	publisher = {IEEE},
	author = {Nizamli, Yasser and Filatov, Anton Yu. and Fadel, Weaam and Shichkina, Yulia A.},
	month = jun,
	year = {2024},
	pages = {170--173},
}

@inproceedings{sharma_neurospectra_2023,
	address = {Raipur, India},
	title = {{NeuroSpectra}: {An} {Innovative} {Multi}-{Class} {Alzheimer}'s {Disease} {Classification} with {ConvNeXt} {Transfer} {Learning} {Model}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350330915},
	shorttitle = {{NeuroSpectra}},
	url = {https://ieeexplore.ieee.org/document/10489677/},
	doi = {10.1109/ICAIIHI57871.2023.10489677},
	abstract = {The disease known as Alzheimer's is a disorder of the nervous system that can be quite devastating. It is characterized by the gradual degeneration of brain cells, leading to the impairment of memory, cognitive function, and the ability to perform daily tasks. While a cure for Alzheimer's remains unclear the timely identification of the disease is of utmost importance in effectively managing its symptoms. Machine learning, specifically deep convolutional neural network methods, enhanced by transfer learning strategies, have emerged as powerful tools for tackling the complex issues associated with analyzing brain imaging data, particularly concerning Alzheimer's disease. The main aim of this categorization study is to encourage innovation and technology for long-term growth in the healthcare sector. This paper introduces a Convolutional Neural Network model built upon the ConvNeXt architecture. This proposed model can classify the brain images into four classes. Alzheimer’s dataset has been utilized for conducting this brain image categorization task. This dataset includes 6400 images belonging to four classes demented, mild, very mild and non demented. The model has demonstrated a high level of accuracy, with a rate of 98.89\% in classifying the images. The achieved values for the model's precision, recall, and F1-score are 99.07\%, 98.99\%, and 0.99, respectively. This research can be utilized in the medical field. By enhancing the early identification and monitoring of this disease, the research advances healthcare access.},
	language = {en},
	urldate = {2024-10-29},
	booktitle = {2023 {International} {Conference} on {Artificial} {Intelligence} for {Innovations} in {Healthcare} {Industries} ({ICAIIHI})},
	publisher = {IEEE},
	author = {Sharma, Gunjan and Anand, Vatsala and Malhotra, Sonal and Kukreti, Sanjeev and Gupta, Sheifali},
	month = dec,
	year = {2023},
	pages = {1--6},
}

@inproceedings{devanshi_early_2023,
	address = {Noida, India},
	title = {An {Early} diagnosis of diabetic retinopathy using {ConvNeXt}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-66549-099-3},
	url = {https://ieeexplore.ieee.org/document/10116214/},
	doi = {10.1109/SPIN57001.2023.10116214},
	abstract = {Diabetic retinopathy (DR), a frequent complication of diabetes mellitus, is the prime reason for vision loss worldwide. The current method for identifying diabetic retinopathy is a time-consuming and manual process, requiring trained medical professionals to examine digital color fundus photographs of the retina and look for the presence of lesions associated with the disease’s vascular abnormalities. Despite technological advancements, accurately identifying the early stages of diabetic retinopathy remains a challenge. To address this issue, different computer vision-based approaches have been introduced to detect DR and classify various stages from retina images. However, these techniques have limited success, particularly in accurately identifying the early stages of the disease. In this study, we employed a publicly available Kaggle Eyepac dataset to train our model for the detection of DR which is known to have ﬁve distinct stages, namely, proliferate DR, severe DR, moderate DR, mild DR, and no DR. The overall accuracy, precision, recall, F1-score, and AUC score obtained by our proposed model were 88.31± 0.23, 89.15± 0.19, 90.25± 0.41, 89.69±0.25, and 87.38± 1.45 respectively. The performance of our model is improved by employing a pre-processing technique known as Contrast Limited Adaptive Histogram Equalization (CLAHE) followed by a stateof-the-art deep learning model known as ConvNeXt to achieve promising results.},
	language = {en},
	urldate = {2024-10-29},
	booktitle = {2023 10th {International} {Conference} on {Signal} {Processing} and {Integrated} {Networks} ({SPIN})},
	publisher = {IEEE},
	author = {{Devanshi} and Baliarsingh, Santos Kumar and Dev, Prabhu Prasad},
	month = mar,
	year = {2023},
	pages = {739--743},
}

@inproceedings{panthakkan_unleashing_2023,
	address = {Tokyo, Japan},
	title = {Unleashing the {Power} of {EfficientNet}-{ConvNeXt} {Concatenation} for {Brain} {Tumor} {Classification}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {9798350345247},
	url = {https://ieeexplore.ieee.org/document/10322070/},
	doi = {10.1109/BMEiCON60347.2023.10322070},
	abstract = {Brain tumors, impacting a substantial global population annually, necessitate precise detection and classification for timely intervention and effective therapy. Though deep learning models have exhibited potential in medical image interpretation, a demand persists for enhanced accuracy and efficiency. This study introduces an optimized solution for brain tumor detection and classification via a concatenated EfficientNet-ConvNeXt model. This novel approach merges the power of EfficientNet and ConvNeXt—two formidable neural networks—to attain extraordinary precision in categorizing various brain tumor types, namely glioma, meningioma, pituitary tumor, and non-tumor. Experimental evaluations validate the model’s superiority over standalone architectures and existing deep learning techniques in terms of accuracy, sensitivity, and specificity. Demonstrating robustness against image quality fluctuations and variability in tumor types, the model exhibits strong potential for realworld clinical usage. Implementation of our proposed concatenated EfficientNet-ConvNeXt model resulted in substantial performance elevation, achieving an exceptional 99\% predictive accuracy. These findings underscore our approach’s accuracy and efficiency, offering substantial aid to radiologists and clinicians in early-stage brain tumor detection and classification. The model’s predictive capabilities can considerably influence patient prognosis and therapy planning through enabling early intervention.},
	language = {en},
	urldate = {2024-10-29},
	booktitle = {2023 15th {Biomedical} {Engineering} {International} {Conference} ({BMEiCON})},
	publisher = {IEEE},
	author = {Panthakkan, Alavikunhu and Anzar, S M and Mansoor, Wathiq},
	month = oct,
	year = {2023},
	pages = {1--5},
}