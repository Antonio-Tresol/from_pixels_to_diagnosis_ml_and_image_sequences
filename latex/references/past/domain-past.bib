% Convolutional neural network combined with attention model for early gastric cancer detection based on images (added by Enrique)
@INPROCEEDINGS{9728413,
  author={Li, Xinyi and Chai, Yi and Zhang, Ke and Chen, Weiqing and Huang, Pengfei},
  booktitle={2021 China Automation Congress (CAC)}, 
  title={Early gastric cancer detection based on the combination of convolutional neural network and attention mechanism}, 
  year={2021},
  volume={},
  number={},
  pages={5731-5735},
  keywords={Sensitivity;Shape;Endoscopes;Streaming media;Cancer detection;Real-time systems;Convolutional neural networks;early gastric cancer;data detection;feature fusion;YOLOv4;CBAM;EGCD},
  doi={10.1109/CAC53003.2021.9728413}
}

% Gastric cancer detection using cnn with transfer learning (added by Enrique)
@INPROCEEDINGS{8513274,
  author={Sakai, Y. and Takemoto, S. and Hori, K. and Nishimura, M. and Ikematsu, H. and Yano, T. and Yokota, H.},
  booktitle={2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)}, 
  title={Automatic detection of early gastric cancer in endoscopic images using a transferring convolutional neural network}, 
  year={2018},
  volume={},
  number={},
  pages={4138-4141},
  keywords={Cancer;Training;Lesions;Gastroenterology;Endoscopes;Feature extraction;Sensitivity},
  doi={10.1109/EMBC.2018.8513274}
}

% Main japanese paper. Exact same problem as ours (added by Enrique).
@article{KentaIshihara2016,
  title={[Paper] Classification of Gastric Cancer Risk From X-ray Images Based on Efficient Image Features Related to Serum Hp Antibody Level and Serum PG Levels},
  author={Kenta Ishihara and Takahiro Ogawa and Miki Haseyama},
  journal={ITE Transactions on Media Technology and Applications},
  volume={4},
  number={4},
  pages={337-348},
  year={2016},
  doi={10.3169/mta.4.337}
}

% Autoencoder for stomach tumors X-RAY (added by Enrique).
@INPROCEEDINGS{8640959,
  author={Tanaka, Makoto and Isokawa, Teijiro and Matsui, Nobuyuki and Yumoto, Takayuki and Kamiura, Naotake},
  booktitle={2018 Joint 7th International Conference on Informatics, Electronics & Vision (ICIEV) and 2018 2nd International Conference on Imaging, Vision & Pattern Recognition (icIVPR)}, 
  title={A Convolutional Autoencoder for Detecting Tumors in Double Contrast X-ray Images}, 
  year={2018},
  volume={},
  number={},
  pages={384-387},
  keywords={Tumors;X-ray imaging;Stomach;Training;Diagnostic radiography;Neural networks;Image reconstruction},
  doi={10.1109/ICIEV.2018.8640959}
}

% MLP with preprocessing methods (added by Enrique)
@INPROCEEDINGS{6032980,
  author={Abe, Koji and Nobuoka, Tetsuya and Minami, Masahide},
  booktitle={Proceedings of 2011 IEEE Pacific Rim Conference on Communications, Computers and Signal Processing}, 
  title={Computer-aided diagnosis of mass screenings for gastric cancer using double contrast X-ray images}, 
  year={2011},
  volume={},
  number={},
  pages={708-713},
  keywords={X-ray imaging;Stomach;Feature extraction;Cancer;Noise;Design automation;Educational institutions},
  doi={10.1109/PACRIM.2011.6032980}
}

% Vision transformer for single images (added by Enrique)
@INPROCEEDINGS{10471994,
  author={Theodore Armand, Tagne Poupi and Bhattacharjee, Subrata and Kim, Hyun-Joong and Hussain, Ali and Ali, Sikandar and Choi, Heung-Kook and Kim, Hee-Cheol},
  booktitle={2024 26th International Conference on Advanced Communications Technology (ICACT)}, 
  title={Vision Transformer-based Model for Gastric Cancer Detection and Classification using Weakly Annotated Histopathological Images}, 
  year={2024},
  volume={},
  number={},
  pages={413-418},
  keywords={Learning systems;Stomach;Image segmentation;Computational modeling;Decision making;Biopsy;Parallel processing;Gastric Cancer;Vision Transformers;Whole Slide Images;Weakly Annotated Images},
  doi={10.23919/ICACT60172.2024.10471994}
}

% Ensemble models for multiple patholofies (added by Enrique)
@INPROCEEDINGS{10055310,
  author={Faruk, Md. Farukuzzaman and Islam, Md. Rabiul and Hashi, Emrana Kabir},
  booktitle={2022 25th International Conference on Computer and Information Technology (ICCIT)}, 
  title={Screening Pathological Abnormalities in Gastrointestinal Images Using Deep Ensemble Transfer Learning}, 
  year={2022},
  volume={},
  number={},
  pages={230-235},
  keywords={Pathology;Transfer learning;Ducts;Gastrointestinal tract;Information technology;Cancer;Gastrointestinal-images;Pathological;Polyps;Compound-scaling;Transfer-learning;Ensemble-networks},
  doi={10.1109/ICCIT57492.2022.10055310}
}


% Similar problem, on lungs with ultrasounds
@INPROCEEDINGS{10208844,
  author={Shea, Daniel E and Kulhare, Sourabh and Millin, Rachel and Laverriere, Zohreh and Mehanian, Courosh and Delahunt, Charles B and Banik, Dipayan and Zheng, Xinliang and Zhu, Meihua and Ji, Ye and Ostbye, Travis and Mehanian, Martha-Marie S and Uwajeh, Atinuke and Akinsete, Adeseye M and Wang, Fen and Horning, Matthew P},
  booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)}, 
  title={Deep Learning Video Classification of Lung Ultrasound Features Associated with Pneumonia}, 
  year={2023},
  volume={},
  number={},
  pages={3103-3112},
  keywords={Deep learning;Computer vision;Pediatrics;Ultrasonic imaging;Machine learning algorithms;Pulmonary diseases;Point of care},
  doi={10.1109/CVPRW59228.2023.00312}
}

% Uses time-spatial distributed 3D CNN to improve accuracy in classifying videos involving medical images of the heart in 14 categories. (added by Antonio)
@article{JMAI5205,
	author = {James P. Howard and Jeremy Tan and Matthew J. Shun-Shin and Dina Mahdi and Alexandra N. Nowbar and Ahran D. Arnold and Yousif Ahmad and Peter McCartney and Massoud Zolgharni and Nick W. F. Linton and Nilesh Sutaria and Bushra Rana and Jamil Mayet and Daniel Rueckert and Graham D. Cole and Darrel P. Francis},
	title = {Improving ultrasound video classification: an evaluation of novel deep learning methods in echocardiography},
	journal = {Journal of Medical Artificial Intelligence},
	volume = {3},
	number = {0},
	year = {2019},
	keywords = {},
	abstract = {Echocardiography is the commonest medical ultrasound examination, but automated interpretation is challenging and hinges on correct recognition of the ‘view’ (imaging plane and orientation). Current state-of-the-art methods for identifying the view computationally involve 2-dimensional convolutional neural networks (CNNs), but these merely classify individual frames of a video in isolation, and ignore information describing the movement of structures throughout the cardiac cycle. Here we explore the efficacy of novel CNN architectures, including time-distributed networks and two-stream networks, which are inspired by advances in human action recognition. We demonstrate that these new architectures more than halve the error rate of traditional CNNs from 8.1% to 3.9%. These advances in accuracy may be due to these networks’ ability to track the movement of specific structures such as heart valves throughout the cardiac cycle. Finally, we show the accuracies of these new state-of-the-art networks are approaching expert agreement (3.6% discordance), with a similar pattern of discordance between views.},
	issn = {2617-2496},	
        url = {https://jmai.amegroups.org/article/view/5205}
}

% they used a lstm and a cnn to classify short ultrasound videos with high accuracy (added by Antonio)
@INPROCEEDINGS{10208844,
  author={Shea, Daniel E and Kulhare, Sourabh and Millin, Rachel and Laverriere, Zohreh and Mehanian, Courosh and Delahunt, Charles B and Banik, Dipayan and Zheng, Xinliang and Zhu, Meihua and Ji, Ye and Ostbye, Travis and Mehanian, Martha-Marie S and Uwajeh, Atinuke and Akinsete, Adeseye M and Wang, Fen and Horning, Matthew P},
  booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)}, 
  title={Deep Learning Video Classification of Lung Ultrasound Features Associated with Pneumonia}, 
  year={2023},
  volume={},
  number={},
  pages={3103-3112},
  keywords={Deep learning;Computer vision;Pediatrics;Ultrasonic imaging;Machine learning algorithms;Pulmonary diseases;Point of care},
  doi={10.1109/CVPRW59228.2023.00312}
}

% técnica de transfer learning basada en el diagnostico de cáncer gástrico.
Basado en endoscopías y no radiografías (Added by Kenneth)
@Article{cancers14225661,
    AUTHOR = {Alrowais, Fadwa and S. Alotaibi, Saud and Marzouk, Radwa and S. Salama, Ahmed and Rizwanullah, Mohammed and Zamani, Abu Sarwar and Atta Abdelmageed, Amgad and I. Eldesouki, Mohamed},
    TITLE = {Manta Ray Foraging Optimization Transfer Learning-Based Gastric Cancer Diagnosis and Classification on Endoscopic Images},
    JOURNAL = {Cancers},
    VOLUME = {14},
    YEAR = {2022},
    NUMBER = {22},
    ARTICLE-NUMBER = {5661},
    URL = {https://www.mdpi.com/2072-6694/14/22/5661},
    PubMedID = {36428752},
    ISSN = {2072-6694},
    ABSTRACT = {Gastric cancer (GC) diagnoses using endoscopic images have gained significant attention in the healthcare sector. The recent advancements of computer vision (CV) and deep learning (DL) technologies pave the way for the design of automated GC diagnosis models. Therefore, this study develops a new Manta Ray Foraging Optimization Transfer Learning technique that is based on Gastric Cancer Diagnosis and Classification (MRFOTL-GCDC) using endoscopic images. For enhancing the quality of the endoscopic images, the presented MRFOTL-GCDC technique executes the Wiener filter (WF) to perform a noise removal process. In the presented MRFOTL-GCDC technique, MRFO with SqueezeNet model is used to derive the feature vectors. Since the trial-and-error hyperparameter tuning is a tedious process, the MRFO algorithm-based hyperparameter tuning results in enhanced classification results. Finally, the Elman Neural Network (ENN) model is utilized for the GC classification. To depict the enhanced performance of the presented MRFOTL-GCDC technique, a widespread simulation analysis is executed. The comparison study reported the improvement of the MRFOTL-GCDC technique for endoscopic image classification purposes with an improved accuracy of 99.25%.},
    DOI = {10.3390/cancers14225661}
}

% Refined Stochastic Data Augmentation and Hard Boundary Box Training for screening (added by Kenneth)
@misc{okamoto2024practicalxraygastriccancer,
      title={Practical X-ray Gastric Cancer Screening Using Refined Stochastic Data Augmentation and Hard Boundary Box Training}, 
      author={Hideaki Okamoto and Quan Huu Cap and Takakiyo Nomura and Kazuhito Nabeshima and Jun Hashimoto and Hitoshi Iyatomi},
      year={2024},
      eprint={2108.08158},
      archivePrefix={arXiv},
      primaryClass={eess.IV},
      url={https://arxiv.org/abs/2108.08158}, 
}

% Compara el desempeño de dos modelos de aprendizaje automático con deep learning en el diagnóstico de cáncer gástrico mediante imágenes médicas. (added by Kenneth)
@article{XIE2023100602,
title = {Deep learning-based gastric cancer diagnosis and clinical management},
journal = {Journal of Radiation Research and Applied Sciences},
volume = {16},
number = {3},
pages = {100602},
year = {2023},
issn = {1687-8507},
doi = {https://doi.org/10.1016/j.jrras.2023.100602},
url = {https://www.sciencedirect.com/science/article/pii/S1687850723000808},
author = {Keping Xie and Jidong Peng},
keywords = {Gastric carcinoma, Deep learning, Pathological methods, Health, Monitoring, GoogLeNet, AlexNet, CNN},
abstract = {Background and objective
Gastric cancer is a kind of tumor with high morbidity and mortality, which seriously threatens people's health and life. It is of great significance to study the early diagnosis and screening of cancer for improving the cure rate of cancer, prolonging the survival time of patients, and reducing the economic and mental burden of patients.
Methodology
Because deep convolutional neural networks can effectively extract deep features of images, and gooenet and AlexNet models can perform wonderful image classification, they are selected for the diagnosis of pathological images of gastric cancer. Moreover, the GooleNet model is optimized to make it more targeted at medical pathological images, which not only ensures the diagnostic accuracy, but also significantly reduces the computational burden.
Results
The improved model has the characteristics of two kinds of network structure at the same time, and is more targeted at gastric cancer pathological sections, improving the sensitivity of gastric cancer pathological section recognition. The results show that the structure has splendid diagnostic accuracy and sensitivity up to 97. 61%, with a specificity of 99. 47 percent.
Conclusion
The optimized model can diagnose gastric cancer more accurately, reduce the possibility of misdiagnosis and missed diagnosis due to doctors' personal reasons, and also help nurses to care and monitor patients, making the whole diagnosis and treatment process more intelligent and safe.}
}

% Clasificación multicategoría utilizando un modelo híbrido de CNN y transformers con imagenes médicas (added by Kenneth)
@article{10.1016/j.cmpb.2022.106924,
author = {Fu, Bangkang and Zhang, Mudan and He, Junjie and Cao, Ying and Guo, Yuchen and Wang, Rongpin},
title = {StoHisNet: A hybrid multi-classification model with CNN and Transformer for gastric pathology images},
year = {2022},
issue_date = {Jun 2022},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {221},
number = {C},
issn = {0169-2607},
url = {https://doi.org/10.1016/j.cmpb.2022.106924},
doi = {10.1016/j.cmpb.2022.106924},
journal = {Comput. Methods Prog. Biomed.},
month = {jun},
numpages = {9},
keywords = {Multi-classification, Gastric cancer, Digital pathology, Deep learning, Transformer}
}

% Nuevo método de aprendizaje supervisado para la detección de gastritis utilizando imágenes de rayos x. (Added by Kenneth)
@article{DBLP:journals/corr/abs-2104-02864,
  author       = {Guang Li and
                  Ren Togo and
                  Takahiro Ogawa and
                  Miki Haseyama},
  title        = {Self-Supervised Learning for Gastritis Detection with Gastric X-Ray
                  Images},
  journal      = {CoRR},
  volume       = {abs/2104.02864},
  year         = {2021},
  url          = {https://arxiv.org/abs/2104.02864},
  eprinttype    = {arXiv},
  eprint       = {2104.02864},
  timestamp    = {Mon, 04 Apr 2022 16:00:10 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2104-02864.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

% Método de deep learning para la clasificación de Covid-19 en base a videos de ultrasonidos pulmonares. (Added by Kenneth)
@article{ERFANIANEBADI2021100687,
    title = {Automated detection of pneumonia in lung ultrasound using deep video classification for COVID-19},
    journal = {Informatics in Medicine Unlocked},
    volume = {25},
    pages = {100687},
    year = {2021},
    issn = {2352-9148},
    doi = {https://doi.org/10.1016/j.imu.2021.100687},
    url = {https://www.sciencedirect.com/science/article/pii/S2352914821001714},
    author = {Salehe {Erfanian Ebadi} and Deepa Krishnaswamy and Seyed Ehsan Seyed Bolouri and Dornoosh Zonoobi and Russell Greiner and Nathaniel Meuser-Herr and Jacob L. Jaremko and Jeevesh Kapur and Michelle Noga and Kumaradevan Punithakumar},
    keywords = {COVID-19, Lung ultrasound, Video classification, Convolutional neural networks},
    abstract = {There is a crucial need for quick testing and diagnosis of patients during the COVID-19 pandemic. Lung ultrasound is an imaging modality that is cost-effective, widely accessible, and can be used to diagnose acute respiratory distress syndrome in patients with COVID-19. It can be used to find important characteristics in the images, including A-lines, B-lines, consolidation, and pleural effusion, which all inform the clinician in monitoring and diagnosing the disease. With the use of portable ultrasound transducers, lung ultrasound images can be easily acquired, however, the images are often of poor quality. They often require an expert clinician interpretation, which may be time-consuming and is highly subjective. We propose a method for fast and reliable interpretation of lung ultrasound images by use of deep learning, based on the Kinetics-I3D network. Our learned model can classify an entire lung ultrasound scan obtained at point-of-care, without requiring the use of preprocessing or a frame-by-frame analysis. We compare our video classifier against ground truth classification annotations provided by a set of expert radiologists and clinicians, which include A-lines, B-lines, consolidation, and pleural effusion. Our classification method achieves an accuracy of 90% and an average precision score of 95% with the use of 5-fold cross-validation. The results indicate the potential use of automated analysis of portable lung ultrasound images to assist clinicians in screening and diagnosing patients.}
}

% Revisión sistemática de literatura acerca del avance de la inteligencia artificial en relación con el cáncer gástrico. (Added by Kenneth)
@Article{diagnostics13243613,
    AUTHOR = {Klang, Eyal and Soroush, Ali and Nadkarni, Girish N. and Sharif, Kassem and Lahat, Adi},
    TITLE = {Deep Learning and Gastric Cancer: Systematic Review of AI-Assisted Endoscopy},
    JOURNAL = {Diagnostics},
    VOLUME = {13},
    YEAR = {2023},
    NUMBER = {24},
    ARTICLE-NUMBER = {3613},
    URL = {https://www.mdpi.com/2075-4418/13/24/3613},
    PubMedID = {38132197},
    ISSN = {2075-4418},
    ABSTRACT = {Background: Gastric cancer (GC), a significant health burden worldwide, is typically diagnosed in the advanced stages due to its non-specific symptoms and complex morphological features. Deep learning (DL) has shown potential for improving and standardizing early GC detection. This systematic review aims to evaluate the current status of DL in pre-malignant, early-stage, and gastric neoplasia analysis. Methods: A comprehensive literature search was conducted in PubMed/MEDLINE for original studies implementing DL algorithms for gastric neoplasia detection using endoscopic images. We adhered to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. The focus was on studies providing quantitative diagnostic performance measures and those comparing AI performance with human endoscopists. Results: Our review encompasses 42 studies that utilize a variety of DL techniques. The findings demonstrate the utility of DL in GC classification, detection, tumor invasion depth assessment, cancer margin delineation, lesion segmentation, and detection of early-stage and pre-malignant lesions. Notably, DL models frequently matched or outperformed human endoscopists in diagnostic accuracy. However, heterogeneity in DL algorithms, imaging techniques, and study designs precluded a definitive conclusion about the best algorithmic approach. Conclusions: The promise of artificial intelligence in improving and standardizing gastric neoplasia detection, diagnosis, and segmentation is significant. This review is limited by predominantly single-center studies and undisclosed datasets used in AI training, impacting generalizability and demographic representation. Further, retrospective algorithm training may not reflect actual clinical performance, and a lack of model details hinders replication efforts. More research is needed to substantiate these findings, including larger-scale multi-center studies, prospective clinical trials, and comprehensive technical reporting of DL algorithms and datasets, particularly regarding the heterogeneity in DL algorithms and study designs.},
    DOI = {10.3390/diagnostics13243613}
}

% Revisión sistemática de literatura de las herramientas y técnicas de visión computacional en relación con el cáncer gástrico. (Added by Kenneth)
@misc{sun2020reviewcomputervisiongastric,
      title={Review on Computer Vision in Gastric Cancer: Potential Efficient Tools for Diagnosis}, 
      author={Yihua Sun},
      year={2020},
      eprint={2005.09459},
      archivePrefix={arXiv},
      primaryClass={eess.IV},
      url={https://arxiv.org/abs/2005.09459}, 
}

% Revisión sistemática de literatura centrada en los avanzas del deep learning para la clasificación de video. (Added by Kenneth)
@Article{electronics13142732,
    AUTHOR = {Mao, Makara and Lee, Ahyoung and Hong, Min},
    TITLE = {Deep Learning Innovations in Video Classification: A Survey on Techniques and Dataset Evaluations},
    JOURNAL = {Electronics},
    VOLUME = {13},
    YEAR = {2024},
    NUMBER = {14},
    ARTICLE-NUMBER = {2732},
    URL = {https://www.mdpi.com/2079-9292/13/14/2732},
    ISSN = {2079-9292},
    ABSTRACT = {Video classification has achieved remarkable success in recent years, driven by advanced deep learning models that automatically categorize video content. This paper provides a comprehensive review of video classification techniques and the datasets used in this field. We summarize key findings from recent research, focusing on network architectures, model evaluation metrics, and parallel processing methods that enhance training speed. Our review includes an in-depth analysis of state-of-the-art deep learning models and hybrid architectures, comparing models to traditional approaches and highlighting their advantages and limitations. Critical challenges such as handling large-scale datasets, improving model robustness, and addressing computational constraints are explored. By evaluating performance metrics, we identify areas where current models excel and where improvements are needed. Additionally, we discuss data augmentation techniques designed to enhance dataset accuracy and address specific challenges in video classification tasks. This survey also examines the evolution of convolutional neural networks (CNNs) in image processing and their adaptation to video classification tasks. We propose future research directions and provide a detailed comparison of existing approaches using the UCF-101 dataset, highlighting progress and ongoing challenges in achieving robust video classification.},
    DOI = {10.3390/electronics13142732}
}

% ML for tumors
@article{fan2022machine,
  title={Machine learning analysis for the noninvasive prediction of lymphovascular invasion in gastric cancer using PET/CT and enhanced CT-based radiomics and clinical variables},
  author={Fan, Lin and Li, Jing and Zhang, Hui and others},
  journal={Abdominal Radiology},
  volume={47},
  number={4},
  pages={1209--1222},
  year={2022},
  publisher={Springer},
  doi={10.1007/s00261-021-03315-1}
}

% AI for gastric cancer
@article{qin2021artificial,
  title={Artificial Intelligence in the Imaging of Gastric Cancer: Current Applications and Future Direction},
  author={Qin, Yuxiang and Deng, Yingqi and Jiang, Haiyan and Hu, Ning and Song, Bin},
  journal={Frontiers in Oncology},
  volume={11},
  pages={631686},
  year={2021},
  publisher={Frontiers Media SA},
  doi={10.3389/fonc.2021.631686},
  pmid={34367946},
  pmcid={PMC8335156}
}